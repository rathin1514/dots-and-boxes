{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 3.5711541175842285,
      "learning_rate": 8e-05,
      "loss": 16.1363,
      "step": 5
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 4.849335193634033,
      "learning_rate": 0.00018,
      "loss": 14.6426,
      "step": 10
    },
    {
      "epoch": 0.025,
      "grad_norm": 6.2091546058654785,
      "learning_rate": 0.00019955307262569833,
      "loss": 13.1912,
      "step": 15
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 9.979484558105469,
      "learning_rate": 0.00019899441340782124,
      "loss": 12.1022,
      "step": 20
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 6.1778669357299805,
      "learning_rate": 0.00019843575418994415,
      "loss": 9.8669,
      "step": 25
    },
    {
      "epoch": 0.05,
      "grad_norm": 3.6861839294433594,
      "learning_rate": 0.00019787709497206705,
      "loss": 8.2445,
      "step": 30
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 2.4500253200531006,
      "learning_rate": 0.00019731843575418996,
      "loss": 7.4248,
      "step": 35
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 2.400646209716797,
      "learning_rate": 0.00019675977653631286,
      "loss": 7.3406,
      "step": 40
    },
    {
      "epoch": 0.075,
      "grad_norm": 1.6476093530654907,
      "learning_rate": 0.00019620111731843577,
      "loss": 6.3148,
      "step": 45
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 1.3938697576522827,
      "learning_rate": 0.00019564245810055868,
      "loss": 6.4944,
      "step": 50
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 0.7604672908782959,
      "learning_rate": 0.00019508379888268158,
      "loss": 5.8578,
      "step": 55
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.5850470066070557,
      "learning_rate": 0.0001945251396648045,
      "loss": 5.7521,
      "step": 60
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 0.6940708756446838,
      "learning_rate": 0.00019396648044692737,
      "loss": 6.2452,
      "step": 65
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 0.34594786167144775,
      "learning_rate": 0.00019340782122905027,
      "loss": 6.0494,
      "step": 70
    },
    {
      "epoch": 0.125,
      "grad_norm": 0.2593991160392761,
      "learning_rate": 0.0001928491620111732,
      "loss": 5.848,
      "step": 75
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.25379058718681335,
      "learning_rate": 0.0001922905027932961,
      "loss": 5.68,
      "step": 80
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 0.352762907743454,
      "learning_rate": 0.000191731843575419,
      "loss": 5.977,
      "step": 85
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.19256633520126343,
      "learning_rate": 0.0001911731843575419,
      "loss": 5.9709,
      "step": 90
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 0.20584841072559357,
      "learning_rate": 0.00019061452513966483,
      "loss": 5.947,
      "step": 95
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.18338114023208618,
      "learning_rate": 0.00019005586592178773,
      "loss": 5.5955,
      "step": 100
    },
    {
      "epoch": 0.175,
      "grad_norm": 0.34421685338020325,
      "learning_rate": 0.0001894972067039106,
      "loss": 5.7258,
      "step": 105
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 0.2566486895084381,
      "learning_rate": 0.00018893854748603352,
      "loss": 5.8435,
      "step": 110
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 0.21824882924556732,
      "learning_rate": 0.00018837988826815642,
      "loss": 5.7194,
      "step": 115
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.1944338083267212,
      "learning_rate": 0.00018782122905027936,
      "loss": 5.522,
      "step": 120
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.2550959587097168,
      "learning_rate": 0.00018726256983240224,
      "loss": 5.9664,
      "step": 125
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 0.18217015266418457,
      "learning_rate": 0.00018670391061452514,
      "loss": 5.8524,
      "step": 130
    },
    {
      "epoch": 0.225,
      "grad_norm": 0.18097947537899017,
      "learning_rate": 0.00018614525139664805,
      "loss": 6.0292,
      "step": 135
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 0.18901382386684418,
      "learning_rate": 0.00018558659217877098,
      "loss": 6.1105,
      "step": 140
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 0.29578742384910583,
      "learning_rate": 0.00018502793296089386,
      "loss": 5.4102,
      "step": 145
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.15515229105949402,
      "learning_rate": 0.00018446927374301676,
      "loss": 5.4597,
      "step": 150
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 0.26835110783576965,
      "learning_rate": 0.00018391061452513967,
      "loss": 5.7899,
      "step": 155
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.24126437306404114,
      "learning_rate": 0.00018335195530726258,
      "loss": 5.6357,
      "step": 160
    },
    {
      "epoch": 0.275,
      "grad_norm": 0.17368467152118683,
      "learning_rate": 0.00018279329608938548,
      "loss": 5.8801,
      "step": 165
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 0.12420177459716797,
      "learning_rate": 0.0001822346368715084,
      "loss": 5.9923,
      "step": 170
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 0.18390019237995148,
      "learning_rate": 0.0001816759776536313,
      "loss": 6.0689,
      "step": 175
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.14064741134643555,
      "learning_rate": 0.0001811173184357542,
      "loss": 5.7325,
      "step": 180
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 0.2307136356830597,
      "learning_rate": 0.0001805586592178771,
      "loss": 5.9552,
      "step": 185
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 0.16480548679828644,
      "learning_rate": 0.00018,
      "loss": 5.4915,
      "step": 190
    },
    {
      "epoch": 0.325,
      "grad_norm": 0.15165849030017853,
      "learning_rate": 0.00017944134078212292,
      "loss": 5.7454,
      "step": 195
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.18019618093967438,
      "learning_rate": 0.00017888268156424582,
      "loss": 5.467,
      "step": 200
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 0.18982191383838654,
      "learning_rate": 0.0001783240223463687,
      "loss": 6.3082,
      "step": 205
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.2149018943309784,
      "learning_rate": 0.00017776536312849163,
      "loss": 5.6881,
      "step": 210
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.19897492229938507,
      "learning_rate": 0.00017720670391061454,
      "loss": 5.9931,
      "step": 215
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 0.18388865888118744,
      "learning_rate": 0.00017664804469273745,
      "loss": 5.5913,
      "step": 220
    },
    {
      "epoch": 0.375,
      "grad_norm": 0.2231505960226059,
      "learning_rate": 0.00017608938547486033,
      "loss": 5.7491,
      "step": 225
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 1.135305404663086,
      "learning_rate": 0.00017553072625698326,
      "loss": 5.808,
      "step": 230
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 0.16417819261550903,
      "learning_rate": 0.00017497206703910616,
      "loss": 5.4969,
      "step": 235
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.19450843334197998,
      "learning_rate": 0.00017441340782122907,
      "loss": 5.904,
      "step": 240
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 0.1633116602897644,
      "learning_rate": 0.00017385474860335195,
      "loss": 5.6584,
      "step": 245
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 0.1956118941307068,
      "learning_rate": 0.00017329608938547485,
      "loss": 5.6946,
      "step": 250
    },
    {
      "epoch": 0.425,
      "grad_norm": 0.19451530277729034,
      "learning_rate": 0.0001727374301675978,
      "loss": 5.4886,
      "step": 255
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 0.3824945390224457,
      "learning_rate": 0.0001721787709497207,
      "loss": 6.0612,
      "step": 260
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 0.22302083671092987,
      "learning_rate": 0.00017162011173184357,
      "loss": 5.7568,
      "step": 265
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.27743929624557495,
      "learning_rate": 0.00017106145251396648,
      "loss": 5.7742,
      "step": 270
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 0.18259023129940033,
      "learning_rate": 0.0001705027932960894,
      "loss": 5.942,
      "step": 275
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.18982256948947906,
      "learning_rate": 0.0001699441340782123,
      "loss": 5.8251,
      "step": 280
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.16024872660636902,
      "learning_rate": 0.0001693854748603352,
      "loss": 5.5861,
      "step": 285
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.1736571192741394,
      "learning_rate": 0.0001688268156424581,
      "loss": 5.7204,
      "step": 290
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.17881925404071808,
      "learning_rate": 0.000168268156424581,
      "loss": 5.7644,
      "step": 295
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.31223273277282715,
      "learning_rate": 0.0001677094972067039,
      "loss": 5.596,
      "step": 300
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 0.2086455374956131,
      "learning_rate": 0.00016715083798882682,
      "loss": 5.5842,
      "step": 305
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 0.285153865814209,
      "learning_rate": 0.00016659217877094972,
      "loss": 5.8101,
      "step": 310
    },
    {
      "epoch": 0.525,
      "grad_norm": 0.22302821278572083,
      "learning_rate": 0.00016603351955307263,
      "loss": 5.8992,
      "step": 315
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.18675750494003296,
      "learning_rate": 0.00016547486033519554,
      "loss": 5.5888,
      "step": 320
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 0.2029915153980255,
      "learning_rate": 0.00016491620111731844,
      "loss": 5.6411,
      "step": 325
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.18461665511131287,
      "learning_rate": 0.00016435754189944135,
      "loss": 5.9149,
      "step": 330
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 0.1991002857685089,
      "learning_rate": 0.00016379888268156425,
      "loss": 5.6274,
      "step": 335
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 0.20007678866386414,
      "learning_rate": 0.00016324022346368716,
      "loss": 5.6729,
      "step": 340
    },
    {
      "epoch": 0.575,
      "grad_norm": 0.18913023173809052,
      "learning_rate": 0.00016268156424581007,
      "loss": 6.3148,
      "step": 345
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 0.24515274167060852,
      "learning_rate": 0.00016212290502793297,
      "loss": 6.0052,
      "step": 350
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 0.23144789040088654,
      "learning_rate": 0.00016156424581005588,
      "loss": 5.9959,
      "step": 355
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.2679624557495117,
      "learning_rate": 0.00016100558659217878,
      "loss": 5.9548,
      "step": 360
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 0.3253627121448517,
      "learning_rate": 0.0001604469273743017,
      "loss": 5.8334,
      "step": 365
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 0.25147905945777893,
      "learning_rate": 0.0001598882681564246,
      "loss": 5.8393,
      "step": 370
    },
    {
      "epoch": 0.625,
      "grad_norm": 0.2289130836725235,
      "learning_rate": 0.0001593296089385475,
      "loss": 5.5129,
      "step": 375
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 0.3127162456512451,
      "learning_rate": 0.00015877094972067038,
      "loss": 5.6191,
      "step": 380
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 0.19359168410301208,
      "learning_rate": 0.0001582122905027933,
      "loss": 5.4542,
      "step": 385
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.22786787152290344,
      "learning_rate": 0.00015765363128491622,
      "loss": 5.4796,
      "step": 390
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 0.2091330736875534,
      "learning_rate": 0.00015709497206703912,
      "loss": 5.4831,
      "step": 395
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.28793030977249146,
      "learning_rate": 0.000156536312849162,
      "loss": 5.6191,
      "step": 400
    },
    {
      "epoch": 0.675,
      "grad_norm": 0.4131273031234741,
      "learning_rate": 0.0001559776536312849,
      "loss": 5.6399,
      "step": 405
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 0.2267579287290573,
      "learning_rate": 0.00015541899441340784,
      "loss": 6.1467,
      "step": 410
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 0.3220280408859253,
      "learning_rate": 0.00015486033519553075,
      "loss": 5.7937,
      "step": 415
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.2635452449321747,
      "learning_rate": 0.00015430167597765363,
      "loss": 5.6635,
      "step": 420
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 0.21939130127429962,
      "learning_rate": 0.00015374301675977653,
      "loss": 5.9186,
      "step": 425
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 0.2702414393424988,
      "learning_rate": 0.00015318435754189946,
      "loss": 5.9482,
      "step": 430
    },
    {
      "epoch": 0.725,
      "grad_norm": 0.23449541628360748,
      "learning_rate": 0.00015262569832402237,
      "loss": 5.7777,
      "step": 435
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.2607553005218506,
      "learning_rate": 0.00015206703910614525,
      "loss": 5.9911,
      "step": 440
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 0.34043100476264954,
      "learning_rate": 0.00015150837988826815,
      "loss": 5.1637,
      "step": 445
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.25875937938690186,
      "learning_rate": 0.00015094972067039106,
      "loss": 5.627,
      "step": 450
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 0.29484274983406067,
      "learning_rate": 0.000150391061452514,
      "loss": 5.832,
      "step": 455
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 0.22719167172908783,
      "learning_rate": 0.00014983240223463687,
      "loss": 5.7497,
      "step": 460
    },
    {
      "epoch": 0.775,
      "grad_norm": 0.19057728350162506,
      "learning_rate": 0.00014927374301675978,
      "loss": 6.0137,
      "step": 465
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 0.34431856870651245,
      "learning_rate": 0.00014871508379888268,
      "loss": 5.5493,
      "step": 470
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 0.2899553179740906,
      "learning_rate": 0.00014815642458100562,
      "loss": 5.8262,
      "step": 475
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.23003000020980835,
      "learning_rate": 0.0001475977653631285,
      "loss": 5.588,
      "step": 480
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 0.32932376861572266,
      "learning_rate": 0.0001470391061452514,
      "loss": 5.402,
      "step": 485
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 0.34490689635276794,
      "learning_rate": 0.0001464804469273743,
      "loss": 5.8414,
      "step": 490
    },
    {
      "epoch": 0.825,
      "grad_norm": 0.23714151978492737,
      "learning_rate": 0.0001459217877094972,
      "loss": 5.8399,
      "step": 495
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.30174651741981506,
      "learning_rate": 0.00014536312849162012,
      "loss": 5.8977,
      "step": 500
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 0.33909672498703003,
      "learning_rate": 0.00014480446927374302,
      "loss": 5.81,
      "step": 505
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.24623936414718628,
      "learning_rate": 0.00014424581005586593,
      "loss": 5.8303,
      "step": 510
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 0.5038164854049683,
      "learning_rate": 0.00014368715083798884,
      "loss": 5.821,
      "step": 515
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.2991400361061096,
      "learning_rate": 0.00014312849162011174,
      "loss": 6.0286,
      "step": 520
    },
    {
      "epoch": 0.875,
      "grad_norm": 0.2508408725261688,
      "learning_rate": 0.00014256983240223465,
      "loss": 5.5067,
      "step": 525
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 0.234602689743042,
      "learning_rate": 0.00014201117318435755,
      "loss": 5.5556,
      "step": 530
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 0.3837429881095886,
      "learning_rate": 0.00014145251396648046,
      "loss": 5.7373,
      "step": 535
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.39462292194366455,
      "learning_rate": 0.00014089385474860334,
      "loss": 5.7084,
      "step": 540
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 0.28556665778160095,
      "learning_rate": 0.00014033519553072627,
      "loss": 5.4853,
      "step": 545
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 0.32607951760292053,
      "learning_rate": 0.00013977653631284918,
      "loss": 5.7863,
      "step": 550
    },
    {
      "epoch": 0.925,
      "grad_norm": 0.34722211956977844,
      "learning_rate": 0.00013921787709497208,
      "loss": 5.6375,
      "step": 555
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.4349463880062103,
      "learning_rate": 0.00013865921787709496,
      "loss": 5.8809,
      "step": 560
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 0.2332112193107605,
      "learning_rate": 0.0001381005586592179,
      "loss": 5.7474,
      "step": 565
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.31409502029418945,
      "learning_rate": 0.0001375418994413408,
      "loss": 5.591,
      "step": 570
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 0.46028339862823486,
      "learning_rate": 0.0001369832402234637,
      "loss": 5.168,
      "step": 575
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 0.4763811528682709,
      "learning_rate": 0.00013642458100558659,
      "loss": 6.046,
      "step": 580
    },
    {
      "epoch": 0.975,
      "grad_norm": 0.4078831374645233,
      "learning_rate": 0.0001358659217877095,
      "loss": 5.9973,
      "step": 585
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 0.29706570506095886,
      "learning_rate": 0.00013530726256983242,
      "loss": 5.8344,
      "step": 590
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 0.25804656744003296,
      "learning_rate": 0.0001347486033519553,
      "loss": 5.6032,
      "step": 595
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.4252542555332184,
      "learning_rate": 0.0001341899441340782,
      "loss": 5.8384,
      "step": 600
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 0.4646400213241577,
      "learning_rate": 0.00013363128491620111,
      "loss": 5.7434,
      "step": 605
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 0.3192639648914337,
      "learning_rate": 0.00013307262569832405,
      "loss": 5.8064,
      "step": 610
    },
    {
      "epoch": 1.025,
      "grad_norm": 0.3434912860393524,
      "learning_rate": 0.00013251396648044693,
      "loss": 5.9142,
      "step": 615
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 0.34177297353744507,
      "learning_rate": 0.00013195530726256983,
      "loss": 5.4995,
      "step": 620
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 0.33348774909973145,
      "learning_rate": 0.00013139664804469274,
      "loss": 5.8357,
      "step": 625
    },
    {
      "epoch": 1.05,
      "grad_norm": 0.31214720010757446,
      "learning_rate": 0.00013083798882681564,
      "loss": 5.8288,
      "step": 630
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 0.2928420603275299,
      "learning_rate": 0.00013027932960893855,
      "loss": 5.5792,
      "step": 635
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.3837476968765259,
      "learning_rate": 0.00012972067039106146,
      "loss": 5.4579,
      "step": 640
    },
    {
      "epoch": 1.075,
      "grad_norm": 0.32883360981941223,
      "learning_rate": 0.00012916201117318436,
      "loss": 5.9748,
      "step": 645
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 0.238523930311203,
      "learning_rate": 0.00012860335195530727,
      "loss": 5.4187,
      "step": 650
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 0.6773896813392639,
      "learning_rate": 0.00012804469273743017,
      "loss": 5.6823,
      "step": 655
    },
    {
      "epoch": 1.1,
      "grad_norm": 0.29444485902786255,
      "learning_rate": 0.00012748603351955308,
      "loss": 5.5612,
      "step": 660
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 0.5789379477500916,
      "learning_rate": 0.00012692737430167598,
      "loss": 5.4593,
      "step": 665
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 0.49178266525268555,
      "learning_rate": 0.0001263687150837989,
      "loss": 5.5404,
      "step": 670
    },
    {
      "epoch": 1.125,
      "grad_norm": 0.4402221739292145,
      "learning_rate": 0.0001258100558659218,
      "loss": 5.9092,
      "step": 675
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.29921090602874756,
      "learning_rate": 0.0001252513966480447,
      "loss": 5.8914,
      "step": 680
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 0.3951128423213959,
      "learning_rate": 0.0001246927374301676,
      "loss": 5.7209,
      "step": 685
    },
    {
      "epoch": 1.15,
      "grad_norm": 0.25901317596435547,
      "learning_rate": 0.0001241340782122905,
      "loss": 5.809,
      "step": 690
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 0.34047457575798035,
      "learning_rate": 0.0001235754189944134,
      "loss": 5.4206,
      "step": 695
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 0.13156363368034363,
      "learning_rate": 0.00012301675977653633,
      "loss": 5.4978,
      "step": 700
    },
    {
      "epoch": 1.175,
      "grad_norm": 0.20986871421337128,
      "learning_rate": 0.00012245810055865923,
      "loss": 5.7798,
      "step": 705
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 0.38406336307525635,
      "learning_rate": 0.00012189944134078212,
      "loss": 5.8546,
      "step": 710
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 0.3426707088947296,
      "learning_rate": 0.00012134078212290503,
      "loss": 5.6009,
      "step": 715
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.6752516031265259,
      "learning_rate": 0.00012078212290502792,
      "loss": 6.2182,
      "step": 720
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 0.22424523532390594,
      "learning_rate": 0.00012022346368715085,
      "loss": 6.0247,
      "step": 725
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 0.42134809494018555,
      "learning_rate": 0.00011966480446927375,
      "loss": 5.6482,
      "step": 730
    },
    {
      "epoch": 1.225,
      "grad_norm": 0.3861202895641327,
      "learning_rate": 0.00011910614525139665,
      "loss": 5.7683,
      "step": 735
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 0.45096662640571594,
      "learning_rate": 0.00011854748603351954,
      "loss": 6.1581,
      "step": 740
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 0.5346019864082336,
      "learning_rate": 0.00011798882681564248,
      "loss": 5.4505,
      "step": 745
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.28095704317092896,
      "learning_rate": 0.00011743016759776537,
      "loss": 5.7794,
      "step": 750
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 0.2800004184246063,
      "learning_rate": 0.00011687150837988828,
      "loss": 5.6642,
      "step": 755
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.29035887122154236,
      "learning_rate": 0.00011631284916201117,
      "loss": 5.3689,
      "step": 760
    },
    {
      "epoch": 1.275,
      "grad_norm": 0.18265491724014282,
      "learning_rate": 0.00011575418994413407,
      "loss": 5.4551,
      "step": 765
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 0.3341388702392578,
      "learning_rate": 0.00011519553072625699,
      "loss": 5.9024,
      "step": 770
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 0.5945308208465576,
      "learning_rate": 0.0001146368715083799,
      "loss": 5.7224,
      "step": 775
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.543785810470581,
      "learning_rate": 0.00011407821229050279,
      "loss": 5.8675,
      "step": 780
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 0.689041018486023,
      "learning_rate": 0.0001135195530726257,
      "loss": 6.0592,
      "step": 785
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 0.3781946003437042,
      "learning_rate": 0.00011296089385474862,
      "loss": 5.729,
      "step": 790
    },
    {
      "epoch": 1.325,
      "grad_norm": 0.21574009954929352,
      "learning_rate": 0.00011240223463687152,
      "loss": 6.0095,
      "step": 795
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.27981823682785034,
      "learning_rate": 0.00011184357541899441,
      "loss": 5.5433,
      "step": 800
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 0.31169381737709045,
      "learning_rate": 0.00011128491620111732,
      "loss": 5.7438,
      "step": 805
    },
    {
      "epoch": 1.35,
      "grad_norm": 0.18375766277313232,
      "learning_rate": 0.00011072625698324024,
      "loss": 5.9491,
      "step": 810
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 0.3054353594779968,
      "learning_rate": 0.00011016759776536315,
      "loss": 5.8544,
      "step": 815
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 0.25126516819000244,
      "learning_rate": 0.00010960893854748604,
      "loss": 5.7224,
      "step": 820
    },
    {
      "epoch": 1.375,
      "grad_norm": 0.27774110436439514,
      "learning_rate": 0.00010905027932960894,
      "loss": 5.6001,
      "step": 825
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 0.31771010160446167,
      "learning_rate": 0.00010849162011173184,
      "loss": 5.6582,
      "step": 830
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 0.7449870109558105,
      "learning_rate": 0.00010793296089385476,
      "loss": 5.6892,
      "step": 835
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.36047765612602234,
      "learning_rate": 0.00010737430167597766,
      "loss": 5.7571,
      "step": 840
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 0.30652931332588196,
      "learning_rate": 0.00010681564245810057,
      "loss": 5.536,
      "step": 845
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 0.2757614850997925,
      "learning_rate": 0.00010625698324022346,
      "loss": 5.858,
      "step": 850
    },
    {
      "epoch": 1.425,
      "grad_norm": 0.4196115732192993,
      "learning_rate": 0.00010569832402234638,
      "loss": 5.8866,
      "step": 855
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 0.5013636350631714,
      "learning_rate": 0.00010513966480446928,
      "loss": 6.1378,
      "step": 860
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 0.2803714871406555,
      "learning_rate": 0.00010458100558659219,
      "loss": 5.7305,
      "step": 865
    },
    {
      "epoch": 1.45,
      "grad_norm": 0.27098792791366577,
      "learning_rate": 0.00010402234636871508,
      "loss": 5.6237,
      "step": 870
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 0.35489553213119507,
      "learning_rate": 0.00010346368715083799,
      "loss": 5.8253,
      "step": 875
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.14617040753364563,
      "learning_rate": 0.00010290502793296091,
      "loss": 5.5373,
      "step": 880
    },
    {
      "epoch": 1.475,
      "grad_norm": 0.3430606424808502,
      "learning_rate": 0.0001023463687150838,
      "loss": 6.018,
      "step": 885
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 0.6639690399169922,
      "learning_rate": 0.0001017877094972067,
      "loss": 5.5741,
      "step": 890
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 0.5967885255813599,
      "learning_rate": 0.00010122905027932961,
      "loss": 5.5659,
      "step": 895
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.12920264899730682,
      "learning_rate": 0.00010067039106145253,
      "loss": 5.5973,
      "step": 900
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 0.36109986901283264,
      "learning_rate": 0.00010011173184357542,
      "loss": 5.7949,
      "step": 905
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 0.2040216326713562,
      "learning_rate": 9.955307262569833e-05,
      "loss": 5.6717,
      "step": 910
    },
    {
      "epoch": 1.525,
      "grad_norm": 0.25402551889419556,
      "learning_rate": 9.899441340782124e-05,
      "loss": 5.6313,
      "step": 915
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.37546882033348083,
      "learning_rate": 9.843575418994414e-05,
      "loss": 5.6311,
      "step": 920
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 0.26173025369644165,
      "learning_rate": 9.787709497206705e-05,
      "loss": 5.7106,
      "step": 925
    },
    {
      "epoch": 1.55,
      "grad_norm": 0.28435495495796204,
      "learning_rate": 9.731843575418995e-05,
      "loss": 5.6341,
      "step": 930
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 0.3149036765098572,
      "learning_rate": 9.675977653631285e-05,
      "loss": 5.7242,
      "step": 935
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 0.2510233521461487,
      "learning_rate": 9.620111731843576e-05,
      "loss": 5.3816,
      "step": 940
    },
    {
      "epoch": 1.575,
      "grad_norm": 0.7338558435440063,
      "learning_rate": 9.564245810055866e-05,
      "loss": 5.8137,
      "step": 945
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 0.6969138383865356,
      "learning_rate": 9.508379888268158e-05,
      "loss": 6.1382,
      "step": 950
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 0.402299165725708,
      "learning_rate": 9.452513966480447e-05,
      "loss": 5.8535,
      "step": 955
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.17333342134952545,
      "learning_rate": 9.396648044692737e-05,
      "loss": 6.016,
      "step": 960
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 0.2907029986381531,
      "learning_rate": 9.340782122905028e-05,
      "loss": 5.6138,
      "step": 965
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 0.13391320407390594,
      "learning_rate": 9.284916201117319e-05,
      "loss": 5.8183,
      "step": 970
    },
    {
      "epoch": 1.625,
      "grad_norm": 0.19249044358730316,
      "learning_rate": 9.229050279329609e-05,
      "loss": 5.9095,
      "step": 975
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 1.0356813669204712,
      "learning_rate": 9.1731843575419e-05,
      "loss": 5.7247,
      "step": 980
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 0.08163072168827057,
      "learning_rate": 9.11731843575419e-05,
      "loss": 5.404,
      "step": 985
    },
    {
      "epoch": 1.65,
      "grad_norm": 0.24480228126049042,
      "learning_rate": 9.061452513966481e-05,
      "loss": 5.3438,
      "step": 990
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 0.17858895659446716,
      "learning_rate": 9.005586592178772e-05,
      "loss": 5.7143,
      "step": 995
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.2838280200958252,
      "learning_rate": 8.949720670391062e-05,
      "loss": 5.7175,
      "step": 1000
    },
    {
      "epoch": 1.675,
      "grad_norm": 0.31159040331840515,
      "learning_rate": 8.893854748603351e-05,
      "loss": 5.4078,
      "step": 1005
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 0.23346783220767975,
      "learning_rate": 8.837988826815643e-05,
      "loss": 5.5909,
      "step": 1010
    },
    {
      "epoch": 1.6916666666666667,
      "grad_norm": 0.21309959888458252,
      "learning_rate": 8.782122905027932e-05,
      "loss": 5.6399,
      "step": 1015
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.32768550515174866,
      "learning_rate": 8.726256983240224e-05,
      "loss": 5.5323,
      "step": 1020
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 0.11689341068267822,
      "learning_rate": 8.670391061452514e-05,
      "loss": 5.5722,
      "step": 1025
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 0.22731129825115204,
      "learning_rate": 8.614525139664806e-05,
      "loss": 5.8177,
      "step": 1030
    },
    {
      "epoch": 1.725,
      "grad_norm": 0.3439755141735077,
      "learning_rate": 8.558659217877095e-05,
      "loss": 5.6178,
      "step": 1035
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.12108548730611801,
      "learning_rate": 8.502793296089387e-05,
      "loss": 5.9374,
      "step": 1040
    },
    {
      "epoch": 1.7416666666666667,
      "grad_norm": 0.3670908212661743,
      "learning_rate": 8.446927374301676e-05,
      "loss": 5.8069,
      "step": 1045
    },
    {
      "epoch": 1.75,
      "grad_norm": 0.3670092821121216,
      "learning_rate": 8.391061452513967e-05,
      "loss": 5.7357,
      "step": 1050
    },
    {
      "epoch": 1.7583333333333333,
      "grad_norm": 0.19352009892463684,
      "learning_rate": 8.335195530726257e-05,
      "loss": 5.5386,
      "step": 1055
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 0.7227962613105774,
      "learning_rate": 8.279329608938548e-05,
      "loss": 5.8097,
      "step": 1060
    },
    {
      "epoch": 1.775,
      "grad_norm": 0.4764322340488434,
      "learning_rate": 8.223463687150838e-05,
      "loss": 5.5559,
      "step": 1065
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 0.0833699107170105,
      "learning_rate": 8.167597765363129e-05,
      "loss": 5.3445,
      "step": 1070
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 0.39860060811042786,
      "learning_rate": 8.11173184357542e-05,
      "loss": 5.6042,
      "step": 1075
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.11153845489025116,
      "learning_rate": 8.05586592178771e-05,
      "loss": 5.4181,
      "step": 1080
    },
    {
      "epoch": 1.8083333333333333,
      "grad_norm": 0.31334570050239563,
      "learning_rate": 8e-05,
      "loss": 5.5979,
      "step": 1085
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 0.186167374253273,
      "learning_rate": 7.944134078212291e-05,
      "loss": 5.8115,
      "step": 1090
    },
    {
      "epoch": 1.825,
      "grad_norm": 0.11771591752767563,
      "learning_rate": 7.88826815642458e-05,
      "loss": 5.6544,
      "step": 1095
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 0.19935506582260132,
      "learning_rate": 7.832402234636872e-05,
      "loss": 5.5846,
      "step": 1100
    },
    {
      "epoch": 1.8416666666666668,
      "grad_norm": 0.5157430768013,
      "learning_rate": 7.776536312849162e-05,
      "loss": 5.8143,
      "step": 1105
    },
    {
      "epoch": 1.85,
      "grad_norm": 0.2390739619731903,
      "learning_rate": 7.720670391061454e-05,
      "loss": 5.4389,
      "step": 1110
    },
    {
      "epoch": 1.8583333333333334,
      "grad_norm": 0.34847089648246765,
      "learning_rate": 7.664804469273743e-05,
      "loss": 5.4922,
      "step": 1115
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.4263065457344055,
      "learning_rate": 7.608938547486035e-05,
      "loss": 6.0266,
      "step": 1120
    },
    {
      "epoch": 1.875,
      "grad_norm": 0.1338365525007248,
      "learning_rate": 7.553072625698324e-05,
      "loss": 5.5882,
      "step": 1125
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 0.3468649089336395,
      "learning_rate": 7.497206703910616e-05,
      "loss": 6.0739,
      "step": 1130
    },
    {
      "epoch": 1.8916666666666666,
      "grad_norm": 0.296258807182312,
      "learning_rate": 7.441340782122905e-05,
      "loss": 5.5753,
      "step": 1135
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.2908407747745514,
      "learning_rate": 7.385474860335197e-05,
      "loss": 6.0141,
      "step": 1140
    },
    {
      "epoch": 1.9083333333333332,
      "grad_norm": 0.10623113065958023,
      "learning_rate": 7.329608938547486e-05,
      "loss": 5.6979,
      "step": 1145
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 0.13941159844398499,
      "learning_rate": 7.273743016759777e-05,
      "loss": 5.9874,
      "step": 1150
    },
    {
      "epoch": 1.925,
      "grad_norm": 0.12356028705835342,
      "learning_rate": 7.217877094972067e-05,
      "loss": 5.9458,
      "step": 1155
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.6280698776245117,
      "learning_rate": 7.162011173184358e-05,
      "loss": 5.6314,
      "step": 1160
    },
    {
      "epoch": 1.9416666666666667,
      "grad_norm": 0.18539440631866455,
      "learning_rate": 7.106145251396649e-05,
      "loss": 5.9864,
      "step": 1165
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.21809379756450653,
      "learning_rate": 7.050279329608939e-05,
      "loss": 5.6274,
      "step": 1170
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 0.3187181055545807,
      "learning_rate": 6.99441340782123e-05,
      "loss": 6.134,
      "step": 1175
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 0.2285338193178177,
      "learning_rate": 6.93854748603352e-05,
      "loss": 5.488,
      "step": 1180
    },
    {
      "epoch": 1.975,
      "grad_norm": 0.1677902340888977,
      "learning_rate": 6.882681564245811e-05,
      "loss": 5.3822,
      "step": 1185
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 0.06998209655284882,
      "learning_rate": 6.826815642458102e-05,
      "loss": 5.4622,
      "step": 1190
    },
    {
      "epoch": 1.9916666666666667,
      "grad_norm": 0.23797115683555603,
      "learning_rate": 6.770949720670391e-05,
      "loss": 5.6627,
      "step": 1195
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.25824323296546936,
      "learning_rate": 6.715083798882681e-05,
      "loss": 5.6909,
      "step": 1200
    },
    {
      "epoch": 2.0083333333333333,
      "grad_norm": 0.3729897439479828,
      "learning_rate": 6.659217877094972e-05,
      "loss": 5.6221,
      "step": 1205
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 0.12296648323535919,
      "learning_rate": 6.603351955307263e-05,
      "loss": 5.6226,
      "step": 1210
    },
    {
      "epoch": 2.025,
      "grad_norm": 0.09284141659736633,
      "learning_rate": 6.547486033519553e-05,
      "loss": 5.4726,
      "step": 1215
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 0.2606416642665863,
      "learning_rate": 6.491620111731844e-05,
      "loss": 5.9469,
      "step": 1220
    },
    {
      "epoch": 2.0416666666666665,
      "grad_norm": 0.24745149910449982,
      "learning_rate": 6.435754189944134e-05,
      "loss": 5.5952,
      "step": 1225
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.16781321167945862,
      "learning_rate": 6.379888268156425e-05,
      "loss": 5.9177,
      "step": 1230
    },
    {
      "epoch": 2.058333333333333,
      "grad_norm": 0.27779582142829895,
      "learning_rate": 6.324022346368715e-05,
      "loss": 5.9277,
      "step": 1235
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.2794647812843323,
      "learning_rate": 6.268156424581006e-05,
      "loss": 5.6423,
      "step": 1240
    },
    {
      "epoch": 2.075,
      "grad_norm": 0.2271765023469925,
      "learning_rate": 6.212290502793297e-05,
      "loss": 5.829,
      "step": 1245
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 0.09480851143598557,
      "learning_rate": 6.156424581005586e-05,
      "loss": 6.0392,
      "step": 1250
    },
    {
      "epoch": 2.091666666666667,
      "grad_norm": 0.3628096580505371,
      "learning_rate": 6.100558659217878e-05,
      "loss": 5.8464,
      "step": 1255
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.43152734637260437,
      "learning_rate": 6.044692737430168e-05,
      "loss": 5.4246,
      "step": 1260
    },
    {
      "epoch": 2.1083333333333334,
      "grad_norm": 0.6606316566467285,
      "learning_rate": 5.988826815642459e-05,
      "loss": 5.6139,
      "step": 1265
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 0.4631061553955078,
      "learning_rate": 5.932960893854749e-05,
      "loss": 5.3832,
      "step": 1270
    },
    {
      "epoch": 2.125,
      "grad_norm": 0.13682913780212402,
      "learning_rate": 5.87709497206704e-05,
      "loss": 5.6259,
      "step": 1275
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.34417805075645447,
      "learning_rate": 5.82122905027933e-05,
      "loss": 5.7698,
      "step": 1280
    },
    {
      "epoch": 2.1416666666666666,
      "grad_norm": 0.5611034035682678,
      "learning_rate": 5.76536312849162e-05,
      "loss": 5.3633,
      "step": 1285
    },
    {
      "epoch": 2.15,
      "grad_norm": 0.08797715604305267,
      "learning_rate": 5.709497206703911e-05,
      "loss": 5.8112,
      "step": 1290
    },
    {
      "epoch": 2.158333333333333,
      "grad_norm": 0.19159354269504547,
      "learning_rate": 5.653631284916201e-05,
      "loss": 5.4988,
      "step": 1295
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 0.20867787301540375,
      "learning_rate": 5.5977653631284924e-05,
      "loss": 5.8568,
      "step": 1300
    },
    {
      "epoch": 2.175,
      "grad_norm": 0.213779479265213,
      "learning_rate": 5.541899441340782e-05,
      "loss": 6.011,
      "step": 1305
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 0.1673642098903656,
      "learning_rate": 5.4860335195530735e-05,
      "loss": 5.5573,
      "step": 1310
    },
    {
      "epoch": 2.191666666666667,
      "grad_norm": 0.6843228340148926,
      "learning_rate": 5.4301675977653634e-05,
      "loss": 5.6407,
      "step": 1315
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.07404506951570511,
      "learning_rate": 5.374301675977654e-05,
      "loss": 5.6471,
      "step": 1320
    },
    {
      "epoch": 2.2083333333333335,
      "grad_norm": 0.2704591155052185,
      "learning_rate": 5.3184357541899446e-05,
      "loss": 5.9466,
      "step": 1325
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 0.17122085392475128,
      "learning_rate": 5.2625698324022345e-05,
      "loss": 5.8341,
      "step": 1330
    },
    {
      "epoch": 2.225,
      "grad_norm": 0.15393699705600739,
      "learning_rate": 5.206703910614526e-05,
      "loss": 5.6174,
      "step": 1335
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.8446287512779236,
      "learning_rate": 5.150837988826816e-05,
      "loss": 5.7731,
      "step": 1340
    },
    {
      "epoch": 2.2416666666666667,
      "grad_norm": 0.06982656568288803,
      "learning_rate": 5.094972067039106e-05,
      "loss": 5.7843,
      "step": 1345
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.11534953862428665,
      "learning_rate": 5.039106145251397e-05,
      "loss": 5.5857,
      "step": 1350
    },
    {
      "epoch": 2.2583333333333333,
      "grad_norm": 0.5747627019882202,
      "learning_rate": 4.9832402234636874e-05,
      "loss": 5.7446,
      "step": 1355
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.07934202998876572,
      "learning_rate": 4.927374301675978e-05,
      "loss": 5.7766,
      "step": 1360
    },
    {
      "epoch": 2.275,
      "grad_norm": 0.20897276699543,
      "learning_rate": 4.8715083798882686e-05,
      "loss": 5.5782,
      "step": 1365
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 0.25840598344802856,
      "learning_rate": 4.8156424581005585e-05,
      "loss": 5.9382,
      "step": 1370
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 0.15228518843650818,
      "learning_rate": 4.759776536312849e-05,
      "loss": 5.8825,
      "step": 1375
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.4073738157749176,
      "learning_rate": 4.7039106145251397e-05,
      "loss": 5.9273,
      "step": 1380
    },
    {
      "epoch": 2.3083333333333336,
      "grad_norm": 0.4316255450248718,
      "learning_rate": 4.64804469273743e-05,
      "loss": 5.7456,
      "step": 1385
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 0.2253858596086502,
      "learning_rate": 4.592178770949721e-05,
      "loss": 5.6932,
      "step": 1390
    },
    {
      "epoch": 2.325,
      "grad_norm": 0.14103741943836212,
      "learning_rate": 4.5363128491620114e-05,
      "loss": 5.8793,
      "step": 1395
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.21408243477344513,
      "learning_rate": 4.480446927374302e-05,
      "loss": 5.5766,
      "step": 1400
    },
    {
      "epoch": 2.341666666666667,
      "grad_norm": 0.47510990500450134,
      "learning_rate": 4.424581005586592e-05,
      "loss": 5.7928,
      "step": 1405
    },
    {
      "epoch": 2.35,
      "grad_norm": 0.2998828887939453,
      "learning_rate": 4.3687150837988825e-05,
      "loss": 5.6644,
      "step": 1410
    },
    {
      "epoch": 2.3583333333333334,
      "grad_norm": 0.09379421919584274,
      "learning_rate": 4.312849162011173e-05,
      "loss": 6.0948,
      "step": 1415
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 0.06934713572263718,
      "learning_rate": 4.2569832402234636e-05,
      "loss": 5.3264,
      "step": 1420
    },
    {
      "epoch": 2.375,
      "grad_norm": 0.09110195189714432,
      "learning_rate": 4.201117318435754e-05,
      "loss": 5.2772,
      "step": 1425
    },
    {
      "epoch": 2.3833333333333333,
      "grad_norm": 0.17840735614299774,
      "learning_rate": 4.145251396648045e-05,
      "loss": 5.722,
      "step": 1430
    },
    {
      "epoch": 2.3916666666666666,
      "grad_norm": 0.14290547370910645,
      "learning_rate": 4.0893854748603354e-05,
      "loss": 5.7727,
      "step": 1435
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.13268519937992096,
      "learning_rate": 4.033519553072626e-05,
      "loss": 5.58,
      "step": 1440
    },
    {
      "epoch": 2.408333333333333,
      "grad_norm": 0.16392403841018677,
      "learning_rate": 3.9776536312849166e-05,
      "loss": 5.6749,
      "step": 1445
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 0.12240947782993317,
      "learning_rate": 3.9217877094972065e-05,
      "loss": 6.0092,
      "step": 1450
    },
    {
      "epoch": 2.425,
      "grad_norm": 0.13883563876152039,
      "learning_rate": 3.865921787709497e-05,
      "loss": 5.9231,
      "step": 1455
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 0.1065639853477478,
      "learning_rate": 3.8100558659217876e-05,
      "loss": 5.6746,
      "step": 1460
    },
    {
      "epoch": 2.4416666666666664,
      "grad_norm": 0.07975199073553085,
      "learning_rate": 3.754189944134078e-05,
      "loss": 5.4982,
      "step": 1465
    },
    {
      "epoch": 2.45,
      "grad_norm": 0.2534591555595398,
      "learning_rate": 3.698324022346369e-05,
      "loss": 5.6701,
      "step": 1470
    },
    {
      "epoch": 2.4583333333333335,
      "grad_norm": 0.1323845386505127,
      "learning_rate": 3.6424581005586594e-05,
      "loss": 5.9426,
      "step": 1475
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.08239050954580307,
      "learning_rate": 3.58659217877095e-05,
      "loss": 5.9084,
      "step": 1480
    },
    {
      "epoch": 2.475,
      "grad_norm": 0.18942832946777344,
      "learning_rate": 3.5307262569832406e-05,
      "loss": 5.9483,
      "step": 1485
    },
    {
      "epoch": 2.4833333333333334,
      "grad_norm": 0.10647531598806381,
      "learning_rate": 3.474860335195531e-05,
      "loss": 5.422,
      "step": 1490
    },
    {
      "epoch": 2.4916666666666667,
      "grad_norm": 0.12543079257011414,
      "learning_rate": 3.418994413407821e-05,
      "loss": 5.5468,
      "step": 1495
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.08754892647266388,
      "learning_rate": 3.3631284916201116e-05,
      "loss": 5.8483,
      "step": 1500
    },
    {
      "epoch": 2.5083333333333333,
      "grad_norm": 0.12415652722120285,
      "learning_rate": 3.307262569832402e-05,
      "loss": 5.8117,
      "step": 1505
    },
    {
      "epoch": 2.5166666666666666,
      "grad_norm": 0.2835424840450287,
      "learning_rate": 3.251396648044693e-05,
      "loss": 5.8562,
      "step": 1510
    },
    {
      "epoch": 2.525,
      "grad_norm": 0.23781834542751312,
      "learning_rate": 3.1955307262569834e-05,
      "loss": 5.5356,
      "step": 1515
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.17371170222759247,
      "learning_rate": 3.139664804469274e-05,
      "loss": 5.6216,
      "step": 1520
    },
    {
      "epoch": 2.5416666666666665,
      "grad_norm": 0.08486136049032211,
      "learning_rate": 3.0837988826815645e-05,
      "loss": 5.8699,
      "step": 1525
    },
    {
      "epoch": 2.55,
      "grad_norm": 0.3738078773021698,
      "learning_rate": 3.027932960893855e-05,
      "loss": 5.7285,
      "step": 1530
    },
    {
      "epoch": 2.5583333333333336,
      "grad_norm": 0.24080611765384674,
      "learning_rate": 2.9720670391061457e-05,
      "loss": 5.8509,
      "step": 1535
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.08826898783445358,
      "learning_rate": 2.9162011173184363e-05,
      "loss": 5.418,
      "step": 1540
    },
    {
      "epoch": 2.575,
      "grad_norm": 0.11842374503612518,
      "learning_rate": 2.8603351955307262e-05,
      "loss": 5.4484,
      "step": 1545
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 0.1120167002081871,
      "learning_rate": 2.8044692737430168e-05,
      "loss": 5.6245,
      "step": 1550
    },
    {
      "epoch": 2.591666666666667,
      "grad_norm": 0.36343634128570557,
      "learning_rate": 2.7486033519553074e-05,
      "loss": 5.8351,
      "step": 1555
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.46578213572502136,
      "learning_rate": 2.692737430167598e-05,
      "loss": 5.436,
      "step": 1560
    },
    {
      "epoch": 2.6083333333333334,
      "grad_norm": 0.12585987150669098,
      "learning_rate": 2.6368715083798885e-05,
      "loss": 5.5893,
      "step": 1565
    },
    {
      "epoch": 2.6166666666666667,
      "grad_norm": 0.11432967334985733,
      "learning_rate": 2.581005586592179e-05,
      "loss": 5.2932,
      "step": 1570
    },
    {
      "epoch": 2.625,
      "grad_norm": 0.08554120361804962,
      "learning_rate": 2.5251396648044694e-05,
      "loss": 5.4213,
      "step": 1575
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 0.3082520365715027,
      "learning_rate": 2.46927374301676e-05,
      "loss": 5.4734,
      "step": 1580
    },
    {
      "epoch": 2.6416666666666666,
      "grad_norm": 0.07337430864572525,
      "learning_rate": 2.4134078212290502e-05,
      "loss": 5.5416,
      "step": 1585
    },
    {
      "epoch": 2.65,
      "grad_norm": 0.14046458899974823,
      "learning_rate": 2.3575418994413408e-05,
      "loss": 5.689,
      "step": 1590
    },
    {
      "epoch": 2.658333333333333,
      "grad_norm": 0.1928442418575287,
      "learning_rate": 2.3016759776536314e-05,
      "loss": 5.5836,
      "step": 1595
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.5202703475952148,
      "learning_rate": 2.245810055865922e-05,
      "loss": 5.411,
      "step": 1600
    },
    {
      "epoch": 2.675,
      "grad_norm": 0.2230960577726364,
      "learning_rate": 2.1899441340782122e-05,
      "loss": 6.0064,
      "step": 1605
    },
    {
      "epoch": 2.6833333333333336,
      "grad_norm": 0.10090967267751694,
      "learning_rate": 2.1340782122905028e-05,
      "loss": 5.4627,
      "step": 1610
    },
    {
      "epoch": 2.6916666666666664,
      "grad_norm": 0.18935313820838928,
      "learning_rate": 2.0782122905027933e-05,
      "loss": 5.5771,
      "step": 1615
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.14777876436710358,
      "learning_rate": 2.022346368715084e-05,
      "loss": 5.9986,
      "step": 1620
    },
    {
      "epoch": 2.7083333333333335,
      "grad_norm": 0.19217252731323242,
      "learning_rate": 1.9664804469273742e-05,
      "loss": 5.6868,
      "step": 1625
    },
    {
      "epoch": 2.716666666666667,
      "grad_norm": 0.13835762441158295,
      "learning_rate": 1.9106145251396648e-05,
      "loss": 6.1588,
      "step": 1630
    },
    {
      "epoch": 2.725,
      "grad_norm": 0.2027830332517624,
      "learning_rate": 1.8547486033519553e-05,
      "loss": 5.5815,
      "step": 1635
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.11392047256231308,
      "learning_rate": 1.798882681564246e-05,
      "loss": 5.4138,
      "step": 1640
    },
    {
      "epoch": 2.7416666666666667,
      "grad_norm": 0.07625807821750641,
      "learning_rate": 1.7430167597765365e-05,
      "loss": 5.3902,
      "step": 1645
    },
    {
      "epoch": 2.75,
      "grad_norm": 0.13690201938152313,
      "learning_rate": 1.6871508379888268e-05,
      "loss": 5.7164,
      "step": 1650
    },
    {
      "epoch": 2.7583333333333333,
      "grad_norm": 0.21946673095226288,
      "learning_rate": 1.6312849162011173e-05,
      "loss": 5.6594,
      "step": 1655
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 0.1374325454235077,
      "learning_rate": 1.575418994413408e-05,
      "loss": 6.0751,
      "step": 1660
    },
    {
      "epoch": 2.775,
      "grad_norm": 0.13622280955314636,
      "learning_rate": 1.5195530726256985e-05,
      "loss": 5.5828,
      "step": 1665
    },
    {
      "epoch": 2.783333333333333,
      "grad_norm": 0.0799490436911583,
      "learning_rate": 1.4636871508379891e-05,
      "loss": 5.5183,
      "step": 1670
    },
    {
      "epoch": 2.7916666666666665,
      "grad_norm": 0.2672678530216217,
      "learning_rate": 1.4078212290502793e-05,
      "loss": 5.7686,
      "step": 1675
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.17552992701530457,
      "learning_rate": 1.3519553072625699e-05,
      "loss": 5.4619,
      "step": 1680
    },
    {
      "epoch": 2.8083333333333336,
      "grad_norm": 0.20562438666820526,
      "learning_rate": 1.2960893854748605e-05,
      "loss": 5.7829,
      "step": 1685
    },
    {
      "epoch": 2.8166666666666664,
      "grad_norm": 0.3145396411418915,
      "learning_rate": 1.2402234636871509e-05,
      "loss": 5.7727,
      "step": 1690
    },
    {
      "epoch": 2.825,
      "grad_norm": 0.1567801684141159,
      "learning_rate": 1.1843575418994413e-05,
      "loss": 5.9161,
      "step": 1695
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.17797940969467163,
      "learning_rate": 1.1284916201117319e-05,
      "loss": 5.6963,
      "step": 1700
    },
    {
      "epoch": 2.841666666666667,
      "grad_norm": 0.2228589504957199,
      "learning_rate": 1.0726256983240223e-05,
      "loss": 5.8344,
      "step": 1705
    },
    {
      "epoch": 2.85,
      "grad_norm": 0.13254253566265106,
      "learning_rate": 1.0167597765363129e-05,
      "loss": 5.9084,
      "step": 1710
    },
    {
      "epoch": 2.8583333333333334,
      "grad_norm": 0.1070299744606018,
      "learning_rate": 9.608938547486033e-06,
      "loss": 5.8099,
      "step": 1715
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.1253376007080078,
      "learning_rate": 9.050279329608939e-06,
      "loss": 5.735,
      "step": 1720
    },
    {
      "epoch": 2.875,
      "grad_norm": 0.18965667486190796,
      "learning_rate": 8.491620111731843e-06,
      "loss": 5.8772,
      "step": 1725
    },
    {
      "epoch": 2.8833333333333333,
      "grad_norm": 0.11565691232681274,
      "learning_rate": 7.932960893854749e-06,
      "loss": 5.1666,
      "step": 1730
    },
    {
      "epoch": 2.8916666666666666,
      "grad_norm": 0.10081072896718979,
      "learning_rate": 7.374301675977655e-06,
      "loss": 5.459,
      "step": 1735
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.18082788586616516,
      "learning_rate": 6.815642458100559e-06,
      "loss": 5.9804,
      "step": 1740
    },
    {
      "epoch": 2.908333333333333,
      "grad_norm": 0.18505075573921204,
      "learning_rate": 6.256983240223464e-06,
      "loss": 5.5313,
      "step": 1745
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 0.675889253616333,
      "learning_rate": 5.698324022346369e-06,
      "loss": 6.0087,
      "step": 1750
    },
    {
      "epoch": 2.925,
      "grad_norm": 0.19771471619606018,
      "learning_rate": 5.139664804469274e-06,
      "loss": 5.9008,
      "step": 1755
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.5516735911369324,
      "learning_rate": 4.581005586592179e-06,
      "loss": 6.0295,
      "step": 1760
    },
    {
      "epoch": 2.9416666666666664,
      "grad_norm": 0.172803595662117,
      "learning_rate": 4.022346368715084e-06,
      "loss": 5.6971,
      "step": 1765
    },
    {
      "epoch": 2.95,
      "grad_norm": 0.09182940423488617,
      "learning_rate": 3.463687150837989e-06,
      "loss": 5.6794,
      "step": 1770
    },
    {
      "epoch": 2.9583333333333335,
      "grad_norm": 0.0633130893111229,
      "learning_rate": 2.905027932960894e-06,
      "loss": 5.5656,
      "step": 1775
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 0.1607227474451065,
      "learning_rate": 2.346368715083799e-06,
      "loss": 5.825,
      "step": 1780
    },
    {
      "epoch": 2.975,
      "grad_norm": 0.15353119373321533,
      "learning_rate": 1.7877094972067039e-06,
      "loss": 5.8491,
      "step": 1785
    },
    {
      "epoch": 2.9833333333333334,
      "grad_norm": 0.22335846722126007,
      "learning_rate": 1.2290502793296089e-06,
      "loss": 6.0899,
      "step": 1790
    },
    {
      "epoch": 2.9916666666666667,
      "grad_norm": 0.11455649137496948,
      "learning_rate": 6.70391061452514e-07,
      "loss": 5.6703,
      "step": 1795
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.12742963433265686,
      "learning_rate": 1.1173184357541899e-07,
      "loss": 5.521,
      "step": 1800
    }
  ],
  "logging_steps": 5,
  "max_steps": 1800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.02946914054144e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
